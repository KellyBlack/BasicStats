
\lecture{Regression Analysis}{regression-analysis}
\section{Regression Analysis}

\title{Regression Analysis}
\subtitle{Analysis Of The Slope Of The Regression Line}

%\author{Kelly Black}
%\institute{Clarkson University}
\date{19 November 2014}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[hideothersubsections,sectionstyle=show/hide]
\end{frame}


\subsection{Clicker Quiz}


\begin{frame}
  \frametitle{Clicker Quiz}

    \iftoggle{clicker}{%

    \begin{columns}
      \column{.25\textwidth}

      \begin{tabular}{l|l}
        $X$ & $Y$ \\ \hline
        1 & 5.0 \\
        2 & 7.1  \\
        3 & 9.0 \\
        4 & 10.9
      \end{tabular}

      \column{.75\textwidth}

      What is the sample correlation?

      \begin{tabular}{l@{\hspace{3em}}l@{\hspace{3em}}l@{\hspace{3em}}l}
        A: 0.0265  & B: 0.1020  \\ C: 0.3922 & D: 0.9997
      \end{tabular}


      \begin{eqnarray*}
        \bar{x} & = & 2.5 \\
        \bar{y} & = & 8.0
      \end{eqnarray*}

    \end{columns}

      \begin{eqnarray*}
        s_{xx} & = & \lp 1-2.5\rp^2 + \lp 2 - 2.5 \rp^2 + \lp 3 - 2.5 \rp^2 + \lp 4 - 2.5 \rp^2 \\
        & = & 5, \\
        s_{yy} & = & \lp 5.0-8.0\rp^2 + \lp 7.1 - 8.0 \rp^2 + \lp 9.0 - 8.0 \rp^2 + \lp 10.9 - 8.0 \rp^2 \\
        & = & 19.22, \\
        s_{xy} & = & \lp 1-2.5\rp\lp 5.0-8.0\rp + \lp 2 - 2.5 \rp\lp 7.1 - 8.0 \rp + \lp 3 - 2.5 \rp\lp 9.0 - 8.0 \rp + \lp 4 - 2.5 \rp\lp 10.9 - 8.0 \rp \\
        & = & 9.8
      \end{eqnarray*}

    \vfill 




    }

\end{frame}

\begin{frame}{Basic Assumptions}
  
  \begin{columns}
    \column{.25\textwidth}

    \begin{tabular}{l|l}
      $X$ & $Y$ \\ \hline
      $x_1$ & $y_1$ \\
      $x_2$ & $y_2$ \\
      $x_3$ & $y_3$ \\
      $x_4$ & $y_4$ \\
      $\vdots$ & $\vdots$ \\
      $x_n$ & $y_n$ \\
    \end{tabular}


    \vfill

    \column{.75\textwidth}

    For linear regression We assume that the relationship between each
    pair of data points is
    \begin{eqnarray*}
      \only<1>{%
        y_i & = & \beta_o + \beta_1 x_i + \epsilon_i,
        }
        \only<2>{%
          y_i & = & \underbrace{\beta_o + \beta_1 x_i}_{\mathrm{linear~relationship}} + 
                    \underbrace{\epsilon_i}_{\mathrm{error}},
        }
    \end{eqnarray*}
    where 
    \begin{eqnarray*}
      \epsilon_i & \thicksim & N(0,\sigma^2)
    \end{eqnarray*}
    is a random variable.

    \vfill

  \end{columns}

\end{frame}


\subsection{Sample Correlation}

\begin{frame}{Calculating the Correlation}

  First define the following sums:
  \begin{eqnarray*}
    S_{xx} & = & (x_1-\bar{x})^2 + (x_2-\bar{x})^2 + \cdots + (x_n-\bar{x})^2, \\
    S_{yy} & = & (y_1-\bar{y})^2 + (y_2-\bar{y})^2 + \cdots + (y_n-\bar{y})^2, \\
    S_{xy} & = & (x_1-\bar{x})(y_1-\bar{y}) + (x_2-\bar{x})(y_2-\bar{y}) + \cdots + (x_n-\bar{x})(y_n-\bar{y}), \\
  \end{eqnarray*}


  \begin{definition}
    The sample correlation coefficient is defined to be
    \begin{eqnarray*}
      r & = & \frac{S_{xy}}{\sqrt{S_{xx} S_{yy}}}.
    \end{eqnarray*}
  \end{definition}

  This definition implies that
  \begin{eqnarray*}
    -1 ~ \leq ~ r ~ \leq 1.
  \end{eqnarray*}
  
\end{frame}


\begin{frame}{Distribution of the Sample Regression}

  \begin{block}{Consequence of the Definition of r}
    The sample correlation follows the distribution
    \begin{eqnarray*}
      t & = & \frac{r\sqrt{n-2}}{\sqrt{1-r^2}}
    \end{eqnarray*}
    with the number of degrees of freedom equal to $n-2$.
  \end{block}
  
\end{frame}




\begin{frame}{Example}

    \begin{columns}
      \column{.25\textwidth}

      \begin{tabular}{l|l}
        $X$ & $Y$ \\ \hline
        1 & 5.0 \\
        2 & 7.1  \\
        3 & 9.0 \\
        4 & 10.9
      \end{tabular}

      \column{.65\textwidth}

      \begin{eqnarray*}
        \bar{x} & = & 2.5 \\
        \bar{y} & = & 8.0 \\
        s_{xx} & = & 5, \\
        s_{yy} & = & 19.22, \\
        s_{xy} & = & 9.8, \\
        r & = & \frac{9.8}{\sqrt{19.22 \cdot 5}}, \\
        & \approx & 0.9997.  
      \end{eqnarray*}

      \end{columns}


      Is there a \redText{linear} relationship?

      \only<2->{%
        We can reject $H_0$ at the 95\% confidence level assuming a
        $t$ distribution with 2 degrees of freedom.
      }
  
\end{frame}


\subsection{Linear Regression}

\begin{frame}{What Is The Relationship?}

  We assume that there is a very good reason to think that the relationship is linear. 
  (i.e. the relationship is motivated by actual science and not just looking at the 
  data.)

  \vfill

  Question: What is the linear relationship?

  Answer: We will construct point estimates for the slope and intercept parameters!

  \vfill
  
\end{frame}

\begin{frame}{Basic Assumptions (Again)}
  
  \begin{columns}
    \column{.25\textwidth}

    \begin{tabular}{l|l}
      $X$ & $Y$ \\ \hline
      $x_1$ & $y_1$ \\
      $x_2$ & $y_2$ \\
      $x_3$ & $y_3$ \\
      $x_4$ & $y_4$ \\
      $\vdots$ & $\vdots$ \\
      $x_n$ & $y_n$ \\
    \end{tabular}


    \vfill

    \column{.75\textwidth}

    For linear regression We assume that the relationship between each
    pair of data points is
    \begin{eqnarray*}
      y_i & = & \beta_o + \beta_1 x_i + \epsilon_i,
    \end{eqnarray*}
    where 
    \begin{eqnarray*}
      \epsilon_i & \thicksim & N(0,\sigma^2)
    \end{eqnarray*}
    is a random variable. \redText{This assumption implies that we
      think that we know the value of each $x_i$ with a high degree of
      certainty.}

    \vfill

  \end{columns}

\end{frame}



\begin{frame}{Linear Regression}

  Given data can I find the ``best'' straight line
  \begin{eqnarray*}
    y & = & \beta_0 + \beta_1 x?
  \end{eqnarray*}


  \only<1>{\centerline{\includegraphics[width=4cm]{img/scatterLR-raw}}}
  \only<2>{\centerline{\includegraphics[width=4cm]{img/scatterLR-line}}}
  \only<3>{\centerline{\includegraphics[width=4cm]{img/scatterLR-preResidual}}}
  \only<4->{%

    \centerline{\includegraphics[width=4cm]{img/scatterLR-Residual}}
    
    \only<5->{%
      Goal: minimize
      \begin{eqnarray*}
        & & \lp\mathrm{residual~1}\rp^2 + \lp\mathrm{residual~2}\rp^2 + \\
        & & \lp\mathrm{residual~3}\rp^2 + \lp\mathrm{residual~4}\rp^2 + \lp\mathrm{residual~5}\rp^2 
      \end{eqnarray*}
    }

  }
  
\end{frame}

\begin{frame}
  \frametitle{Linear Regression}

    \begin{columns}
      \column{.65\textwidth}

      \centerline{\includegraphics[width=6cm]{img/regressionGeneral}}

      \column{.35\textwidth}
      
      Our \textit{approximation} for the best linear fit is
      \begin{eqnarray*}
        y & = & \hat{m} x + \hat{b}.
      \end{eqnarray*}
      where
      \begin{eqnarray*}
        \hat{m} & = & \frac{s_{xy}}{s_{xx}}, \\
        \hat{b} & = & \bar{y} - \hat{m} \bar{x}.
      \end{eqnarray*}

    \end{columns}

\end{frame}


\begin{frame}{The Residual}

  \begin{definition}
    The \textit{\color{red} predicted value} for $y$ at $x_i$ is
    \begin{eqnarray*}
      \hat{y}_i & = & \hat{m} x_i + \hat{b}.
    \end{eqnarray*}

    The \textit{\color{red} residual} at $x_i$ is 
    \begin{eqnarray*}
      \mathrm{Residual} & = & y_i - \lp \hat{m} x_i + \hat{b} \rp, \\
      & = & y_i - \hat{y}_i.
    \end{eqnarray*}

  \end{definition}

  \only<2->
  {

    \begin{definition}
      The sample variance of the error is 
      \begin{eqnarray*}
        \hat{\sigma}^2_y & = & \frac{\lp y_1 - \hat{y}_1\rp^2 + \lp y_2 - \hat{y}_2\rp^2 + \cdots + \lp y_n - \hat{y}_n\rp^2 }{n-2}.
      \end{eqnarray*}
    \end{definition}

  }
  
\end{frame}

\begin{frame}{Example}

    \begin{columns}
      \column{.25\textwidth}

      \begin{tabular}{l|l}
        $X$ & $Y$ \\ \hline
        1 & 2 \\
        2 & 3  \\
        3 & 3 \\
        4 & 5  \\
        5 & 4
      \end{tabular}

      \column{.65\textwidth}

      \only<2>{%
        \centerline{\includegraphics[width=4cm]{img/regressionExample}}
      }

      \only<3>{%
        \centerline{\includegraphics[width=4cm]{img/regressionExampleLine}}
      }


      \only<4->{%
        \begin{eqnarray*}
          \bar{x} & = & 3 \\
          \bar{y} & = & 3.4 \\
          s_{xx} & = & 10, \\
          s_{yy} & = & 5.2, \\
          s_{xy} & = & 6
        \end{eqnarray*}
      }

      \end{columns}

  \only<5->
  {

    \begin{eqnarray*}
      \hat{m} & = & \frac{s_{xy}}{s_{xx}}, \\
      & = & 0.6, \\
      \hat{b} & = & \bar{y} - \hat{m} \bar{x}, \\
      & = & 1.6
    \end{eqnarray*}

  }

  
\end{frame}



\begin{frame}{Example}

    \begin{columns}
      \column{.25\textwidth}

      \begin{tabular}{l|l}
        $X$ & $Y$ \\ \hline
        1 & 2 \\
        2 & 3  \\
        3 & 3 \\
        4 & 5  \\
        5 & 4
      \end{tabular}

      \column{.65\textwidth}

        \begin{eqnarray*}
          \bar{x} & = & 3 \\
          \bar{y} & = & 3.4 \\
          s_{xx} & = & 10, \\
          s_{yy} & = & 5.2, \\
          s_{xx} & = & 6, \\
          \hat{m} & = & 0.6, \\
          \hat{b} & = & 1.6
        \end{eqnarray*}

      \end{columns}

  \only<2->
  {

      \begin{tabular}{r|r<{\onslide<3->}|r<{\onslide<4->}|r<{\onslide}} % 
        $X$ & $Y$ & $\hat{Y}$ & Residual \\ \hline
        1 & 2 & 2.2 & -.2 \\
        2 & 3 & 2.8 &  .2 \\
        3 & 3 & 3.4 & -.4 \\
        4 & 5 & 4.0 & 1.0  \\
        5 & 4 & 4.6 & -.6
      \end{tabular}


  }

  
\end{frame}

\subsection{Inference for the Slope}

\begin{frame}{Example}

  \begin{tabular}{r|r|r|r}
    $X$ & $Y$ & $\hat{Y}$ & Residual \\ \hline
    1 & 2 & 2.2 & -.2 \\
    2 & 3 & 2.8 &  .2 \\
    3 & 3 & 3.4 & -.4 \\
    4 & 5 & 4.0 & 1.0  \\
    5 & 4 & 4.6 & -.6
  \end{tabular}

  \begin{eqnarray*}
    \hat{\sigma}^2_y & = & \frac{ (-.2)^2 + (.2)^2 + (-.4)^2 + (1.0)^2 + (-.6)^2}{5-2}, \\
    & = & \frac{1.6}{3}, \\
    \hat{\sigma}_y & = & \sqrt{\frac{1.6}{3}}, \\
    & \approx & .730
  \end{eqnarray*}
  
\end{frame}
  

\begin{frame}{What is the relationship?}


    \begin{columns}
      \column{.33\textwidth}

      \begin{eqnarray*}
        H_0: & & m=0, \\
        H_a: & & m<0,
      \end{eqnarray*}
      a negative relationship?

      \column{.33\textwidth}

      \begin{eqnarray*}
        H_0: & & m=0, \\
        H_a: & & m>0,
      \end{eqnarray*}
      a positive relationship?


      \column{.33\textwidth}

      \begin{eqnarray*}
        H_0: & & m=0, \\
        H_a: & & m\neq 0,
      \end{eqnarray*}
      any relationship?

      
    \end{columns}

  
\end{frame}





% LocalWords:  Clarkson pausesection hideallsubsections hideothersubsections
% LocalWords:  sectionstyle
